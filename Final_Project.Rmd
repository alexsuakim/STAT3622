---
title: "Nature or Nurture? Estimating Obesity Based on Physical Condition & Eating Habits"
author: "Soo-ah Kim (3035661061), Dongjun Yeom (3035666463)"
date: "2024-04-18"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# Import the necessary libraries

knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(corrplot)
library(knitr)
library(tidyr)
library(dplyr)
library(e1071)
library(caret)
library(class)
library(rpart)
library(rpart.plot)
library(randomForest)
library(moments)
library(coefplot)
library(nnet)
```

# Introduction

*"Nature or Nurture?"* This is one of the most important debated
questions in the field of human biology. Some say genetic and physical
factors play a bigger role, and others say the environments or habits
play a bigger role in shaping a condition in a person. In this report,
we focused on estimating the obesity level based on physical condition
and eating habits. Obesity considered a great threat in the developed
economies, and is an increasing threat in the developing countries.
According to Public Health England, 63% of adults in England were
overweight or obese in 2015. It is an important task to find out what
factors influence the obesity levels most and to predict the obesity
level based on a person's physical condition and eating habits.

We aim to analyse what factors play a bigger role in the obesity level
of a person, using various machine learning models, such as logistic
regression, k-nearest neighbours, naive bayes analysis, support vector
machine, decision tree and random forest.

# About the Data

```{r import_data}
# Import data

data <- read.csv("obesity.csv")
head(data)
```

We employed 2,111 records for the estimation of obesity levels in
individuals from the countries of Mexico, Peru and Colombia, based on
their eating habits and physical condition. The original data was
donated to the UC Irvine Machine Learning Repository and can be found
from the following link.

<https://archive.ics.uci.edu/dataset/544/estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition>

# Attributes

```{r}
# Add BMI into consideration

data$BMI <- with(data, Weight / (Height^2))
head(data)
```

*Note:* The original dataset contained 17 attributes, and 1 attribute
has been added for the analysis.

The following attributes were considered for the analyses:

1.  *The attributes related with the physical condition:*

> -   Gender
> -   Age
> -   Height
> -   Family History with Obesity
> -   Body Mass Index (BMI)

2.  *The attributes related with eating habits:*

> -   Frequent consumption of high caloric food (FAVC)
> -   Frequency of consumption of vegetables (FCVC)
> -   Number of main meals (NCP)
> -   Consumption of food between meals (CAEC)
> -   Whether they smoke cigarettes (SMOKE)
> -   Consumption of water daily (CH20)
> -   Consumption of alcohol (CALC)
> -   Calories consumption monitoring (SCC)
> -   Physical activity frequency (FAF)
> -   Time using technology devices (TUE)
> -   Transportation used (MTRANS)

# Exploratory Data Analysis

## Bar Plot

```{r, warning=FALSE}
#bar chart

# Set the factor levels for obesity level in ascending order
data$NObeyesdad <- factor(data$NObeyesdad,
levels = c("Insufficient_Weight", "Normal_Weight", "Overweight_Level_I", "Overweight_Level_II", "Obesity_Type_I", "Obesity_Type_II", "Obesity_Type_III" ))

# Plot
ggplot(data, aes(x = NObeyesdad, fill = ..count..)) + 
  geom_bar() +
  coord_flip() +
  labs(x = "Count", y = "Obesity Level", title = "Distribution of Obesity Levels") +
  theme_minimal() +
  theme(legend.position = "none")
```

In figure 1, the bar plot above indicates the distribution of the data.
Based on the Body Mass Index calculated, the data was classified into 7
categories:

> -   Insufficient_Weight: Less than 18.5
>
> -   Normal_Weight: 18.5 to 24.9
>
> -   Overweight_Level_I: 25.0 to 27.4
>
> -   Overweight_Level_II: 27.5 to 29.9
>
> -   Obesity_Type_I: 30.0 to 34.9
>
> -   Obesity_Type_II: 35.0 to 39.9
>
> -   Obesity_Type_III: Higher than 40

The distribution of obesity level shows no class imbalance, with each of
the classes having similar numbers of instances all ranging from 250 to
350. This might indicate a sampling bias due to the difference with the
real world distribution, which is more or less bell-curved, as shown in
Figure 2 below (Al-Malki et al., 2003).

However, having less class imbalance might actually work as an advantage
in our analysis. Models trained on balanced data are less likely to
overfit to the majority class. They are more likely to generalize better
to unseen data since they have had the opportunity to learn
representative features from all classes equally.

![Figure 2. Real-world BMI distribution (Al-Malki et al.,
2003)](images/clipboard-488641534.png){width="542"}

## Correlation Plot

```{r}
# correlation heatmap for numeric columns

numeric_columns <- sapply(data, is.numeric) & !names(data) %in% 'NObeyesdad'
numeric_data <- data[numeric_columns]

cor_matrix <- cor(numeric_data)  

corrplot(cor_matrix, method = "color", 
         tl.col = "black",
         addCoef.col = "black")
```

The above correlation plot presents the inter-relationships between a
set of variables. The matrix reveals a positive correlation between
*Weight* and *BMI*, where 0.93 signifies a strong direct relationship.
Similarly, *Height* and *Weight* shows a moderate strong positive
correlation with a coefficient of 0.46, implying that *height* is a
contributing factor to *weight*. This is explainable since *BMI* is
calculated by the following equation:

$$
BMI = \frac {weight(Kg)} {height(m) ^2}
$$

Due to the high correlation, we decided not to include *BMI* as a
variable in the following analyses to prevent multicollinearity
problems.

On the other hand, a moderate negative correlation with a coefficient of
-0.3 is observed between *TUE* and *FAF*, indicating that an increase in
one may correspond with a decrease in the other. For other pairs, they
exhibited relatively weak correlations, coefficients being lower than
0.3 in absolute manner, suggesting negligible linear associations.

For better visualisation, our correlation plot employs the colour scale:
darker shades of blue represent stronger positive correlations and
darker shades of red denote stronger negative correlations.

## Pairwise Plot

```{r}
# pairwise plot of selected columns

ppcor <- function(x, y, ...) {
  points(x, y, ...) 
  abline(lm(y ~ x), col = "red")
}

selected_data <- data[, c("Age", "FCVC", "NCP", "CH2O", "FAF", "BMI")]
pairs(selected_data, panel = ppcor)
```

The above pairwise plot examines the potential correlations between the
selected variables, including *Age, FCVC, NCP, CH2O, FAF*, and *BMI*.
The red linear regression lines within each scatterplot serve as a
reference to estimate the linearity between variables. While the
regression lines are mostly flat for all the scatterplots, suggesting a
weak linear relationship, the *Age* and *BMI* shows a gradual positive
linearity. This further supports the decision to eliminate *BMI* to
prevent multicollinearity. Overall, while some linear relationship may
exist, it is not strong or consistent across all variable pairs. Thus,
other than *BMI*, there are no further variables to remove in order to
prevent strong multicollinearity between bariables.

## QQ-Plots

```{r}
# QQ-plots

# Define the columns you want to plot
columns_to_plot <- c("Age", "Height", "Weight")

# Check if all specified columns exist in the data
if (!all(columns_to_plot %in% names(data))) {
  stop("One or more specified columns do not exist in the dataset.")
}

# Plotting each selected column
par(mfrow = c(1, length(columns_to_plot)))  # Arrange plots in a single row
for (col in columns_to_plot) {
    qqnorm(data[[col]], main = paste("QQ-plot of", col))
    qqline(data[[col]], col = "steelblue")  # Add a reference line
}
par(mfrow = c(1, 1))  # Reset plot layout
```

```{r}
# Shapiro-Wilk Test
shapiro.test(data$Age)

# Kurtosis
kurtosis(data$Age)
```

From the QQ-plot, we can see that the variables *height* and *weight*
generally follows the normal distribution line, but *age* doesn't seem
to quite follow a normal distribution. To check the deviance from normal
distribution of the variable *age*, we performed the Sapiro-Wilk
normality test and the kurtosis test.

#### Shapiro-Wilk Normality Test

The W-statistic value of W = 0.86606 indicates how well the data
conforms to a normal distribution. A value closer to 1 would indicate
data that more closely follows a normal distribution. A value of 0.86606
suggests a noticeable deviation from normality. The p-value \< 2.2e-16
is highly significant, which strongly rejects the null hypothesis that
the data are normally distributed. This result confirms that the
distribution of Age significantly deviates from a normal distribution.

#### Kurtosis

The reported kurtosis value of 5.816858 indicates that the distribution
has heavier tails than a normal distribution (which has a kurtosis of
3). This leptokurtic nature is consistent with the QQ-plot observation
where both tails were above the normal line, suggesting more extreme
values in the tails than expected under normality.

## Box Plots for Numeric and Nominal Variables

```{r}
# Box Plots

# Loop through the numeric columns to create box plots
for (variable_name in names(numeric_data)) {
  p <- ggplot(data, aes_string(x = 'NObeyesdad', y = variable_name, fill = 'NObeyesdad')) +
    geom_boxplot() +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = paste("Box plot of", variable_name, "by NObeyesdad"),
         x = "Obesity Category",
         y = variable_name)

  print(p)
}
```

#### *Box Plot of Age by NObeyesdad*

The median *ages* for all categories are normally in the early twenties,
with a slight upward trend as the categories progress from
*Insufficient_Weight* towards *Obesity_Type_III*. Additionally,
regarding the widening interquartile ranges, the box plot portrays an
increasing variability of *age* distribution in the *overweight* and
*obesese* categories compare to the *Normal* and *Insufficient weights*,
suggesting a greater diversity in the ages of individuals as the
severity of obesity increases. Moreover, the existence of outliers
across all categories indicates that there are ages that deviate
significantly from the median, leading to the complexity and variability
of *age* within classification.

#### *Box Plot of Height by NObeyesdad*

The median *height* is consistent across categories, mostly lying
between approximately 1.65 and 1.75. The IQR, the middle 50% of the
data, are considered stable across categories, proposing a little
variation in *height* regardless of *weight*. The absence of notable
differences in median *height* across *weight* categories shows that
height may not be serve as a strong differentiator of obesity status.

#### *Box Plot of Weight by NObeyesdad*

The box Plot of Weight by NObeyesdad shows an ascending order, median
*weight*s increasing, indicating a positive correlation between the
severity of category and median *weight*. Regarding the IQR, the size of
the box are consistent across categories, except for *Obesity_Type_III*.
Broader IQR may suggest the greater *weight* variation. For
*Overweight_Level_II*, *Obesity_Type_II*, and *Obesity_Type_III*
displays a presence of outliers.

#### *Box Plot of FCVC by NObeyesdad*

The median appears to decrease as the *level of obesity* increases. The
IQR is larger in the Insufficient Weight compare to the other
categories, implying greater variability. Outliers exclusively exist in
the *Obesity_Type_I*, suggesting that individual cases with FCVC
measurements for *Obesity_Type_I* are lower than the others. There is an
apparent descreasing trend of median, but the relationship is not
linear, which can be seen by the increase in median *FCVC* for the
*Obesity_Type_II* compared to *Type_I*.

#### *Box Plot of NCP by NObeyesdad*

The median *NCP* of *Insufficient-Weight* rated 3, with a relatively
compact size of IQR, leading to a less variability. The rest of
categories has shown similar median *NCP* values, close to 3. The IQR
was relatively longer in *Overweight_Level_I, Overweight_Level_II,* and
*Obesity_Type_I*. As most of the categories has exhibited outliers, we
assume that individual cases within *NCP* significatntly differ from the
norm.

#### *Box Plot of CH2O by NObeyesdad*

The median values for each category are mostly consistent, suggesting
that *CH2O* may not notably vary among all the individuals. This
uniformness further suggests that *CH2O* may not serve as a significant
factor influencing the differences in obesity levels. The IQRs are
comparable among the categories, further supports a homogeneous *CH2O*
pattern across the obesity levels.

#### *Box Plot of FAF by NObeyesdad*

The plot illustrates a slight decreasing trend in the median value, with
*Insufficient_Weight* rating the highest median, while
*Obesity_Type_III* having the lowest. The IQRs have shown similarity
across the categories indicating a consistency in the spread of *FAF*
values within each obesity level. The box plot for *Obesity_Type_II*
presents the narrowest IQR with same median as others, implying less
variability within this group, further supporting the possibility that
the individuals with *Obesity_Type_II* may contain a subgroup with
higher physical activity levels. Or else, it may reflect a limitation.
Overall, regarding the box plots, the correlation between lower physical
activity levels and higher *obesity levels* are likely to be promoted.

#### *Box Plot of TUE by NObeyesdad*

The median TUE appears to differ across the obesity levels. The
interquartile range for Obesity_Type_III is narrower compared then
categories, suggesting less variability in TUE among individuals within
the group. The noticable trends, such as the linearity within median, is
not noticeable in this box plot.

#### *Box Plot of BMI by NObeyesdad*

The median *BMI* increases with each category. The IQR within each
category are also significantly narrow. Although some categories
presents the potential outliers, the box plot displays clear correlation
between *BMI* and *obesity level*, supporting that the individuals with
higher *BMI* are likely to fall into higher *obesity*.

## Bar Charts for Binary and Categorical Variables

```{r}
#plot categorical variables by obesity level

#change order of categorical variables
data$CAEC <- factor(data$CAEC, levels = c("no", "Sometimes", "Frequently", "Always"))
data$CALC <- factor(data$CALC, levels = c("no", "Sometimes", "Frequently", "Always"))
data$MTRANS <- factor(data$MTRANS, levels = c("Walking", "Bike", "Motorbike",
                                              "Public_Transportation", "Automobile"))

# Identify the categorical and binary columns
categorical_columns <- sapply(data, function(x) is.factor(x) || is.character(x) || length(unique(x)) == 2)
categorical_data <- data[categorical_columns]

# Loop through the categorical columns to create bar charts
for (variable_name in names(categorical_data)) {
  if (variable_name != "NObeyesdad") {
    p <- ggplot(data, aes_string(x = variable_name, fill = 'NObeyesdad')) +
      geom_bar(position = "fill") + 
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
      labs(title = paste("Bar chart of", variable_name, "by NObeyesdad"),
           x = variable_name,
           y = "Proportion")
  
    print(p)
  }
}
```

#### *Bar Chart of Gender by NObeyesdad*

The stacked bar chart depicts the gender-based distribution on 7
*obesity levels*. The vertical axis represents the proportion of obesity
classifications within *gender*. For females, *Obesity_Type_III*
constitutes the majority, followed by *Insufficient_Weight,* while
*Obesity_Type_II* is predominant in males. Regarding the the smallest
proportions, females barely had *Overweight_II*, while males were hardly
underweight. Overall, the above bar chart clearly illustrates the
*gender* disparities within weight distributions, with a higher
proportion of females in the lower eight category and a higher
proportion of males in the overweight categories.

#### *Bar Chart of Family_history by NObeyesdad*

The bar chart depicts the *family history of obesity* distribution on 7
*obesity levels*. There is a stark difference between the group with
*family history* and the group without. More than 70% of the group with
*family history* are overweight or obese, while less than 30% of the
group without any *family history* are overweight or obese.

#### *Bar Chart of FAVC by NObeyesdad*

The bar chart depicts the *FAVC (frequency of consuming high caloric
food)* distribution on 7 *obesity levels*. There is a clear difference
between the group that does consume high caloric food frequently and the
group that does not. More than 75% of the group that responded that they
consume high caloric food frequently are overweight or obese, while less
than 50% of the group that responded that they don't are overweight or
obese.

#### *Bar Chart of CAEC by NObeyesdad*

The bar chart depicts the *CAEC (consumption of food between meals)*
distribution on 7 *obesity levels*. There is a clear difference between
the four groups of different *CAEC* levels, but the result is
interesting since it is slightly counter-intuitive. *Sometimes* was the
most common answer for samples that are overweight or obese, and for the
responses *frequently* and *always*, the percentage of overweight and
obese samples drop significantly. On the other hand, *frequently* and
*always* were the two most common responses for *normal weight* samples.

#### *Bar Chart of SMOKE by NObeyesdad*

The bar chart depicts the *SMOKE (whether they smoke cigarettes)*
distribution on 7 *obesity levels*. The result is interesting. The group
that does not smoke have almost a uniform number of instances for every
obesity level. However, in the group that does smoke, there are a lot of
*normal_weight* and *obesity_type_II* samples.

#### *Bar Chart of SCC by NObeyesdad*

The bar chart depicts the *SCC (calories consumption monitoring)*
distribution on 7 *obesity levels*. For the group that does not monitor
calorie intake, the number of instances for every obesity level was near
uniform. However, for the group that does track calorie intake, more
than 90% were in the range between *insufficient weight* to
*overweight_level_I*.

#### *Bar Chart of CALC by NObeyesdad*

The bar chart depicts the *CALC (consumption of alcohol)* distribution
on 7 *obesity levels*. An interesting result is that none of the obese
instances were included in the group that always consume alcohol. Most
of them were included in the group that does not or only sometimes drink
alcohol.

#### *Bar Chart of MTRANS by NObeyesdad*

The bar chart depicts the *MTRANS (mode of transportation)* distribution
on 7 *obesity levels*. For the MTRANS group that usually walks, the
overweight to obese instances make up less thatn 30%. For the groups
that usually rides the bicycle or the motorcycle, the overweight to
obese instances make up less than 50%. However, for the groups that
usually take public transportation or the automobile, the percentage
goes up to nearly 75%. We can interpret this as the mode of
transportation plays quite an important role in the obesity level, and
the more active the mode of transportation is, the less obese the
individual is likely to be.

# Data Modelling and Analysis

After performing exploratory data analysis, we performed machine
learning models to classify the obesity level. We first performed
classification by considering the different levels of obesity as
categories. Then we performed some regression methods by considering the
obesity level as ordinal.

## Data Preprocessing

To preprocess the data, we divided the dataset into 80% training set and
20% testing set. To further handle the data, we normalised the numeric
variables and converted factors to dummy variables for the categorical
variables.

```{r}
# Preprocessing

# Preprocess numerical data: normalise, handle categorical variables, etc.
preprocess_params <- preProcess(data[, -which(names(data) %in% c("BMI"))], method = c("center", "scale"))
normalized_data <- predict(preprocess_params, data[, -which(names(data) %in% c("BMI"))])

#train vs test
index <- createDataPartition(normalized_data$NObeyesdad, p = 0.8, list = FALSE)
train_set <- normalized_data[index, ]
test_set <- normalized_data[-index, ]

# X_train & X_test
X_train <- train_set[, -which(names(train_set) == "NObeyesdad")]
X_test <- test_set[, -which(names(test_set) == "NObeyesdad")]
y_train <- train_set[["NObeyesdad"]]
y_test <- test_set[["NObeyesdad"]]


# Convert factors to dummy variables
dummies <- dummyVars(" ~ .", data = X_train)
X_train <- predict(dummies, newdata = X_train)
X_test <- predict(dummies, newdata = X_test)
```

## Multinomial Logistic Regression

```{r, warning=FALSE}
# Multinomial logistic regression 
#(instead of linear regression since the target variable NObeyesdad is categorical)

# Convert all categorical variables to factors
categorical_columns <- sapply(data, function(x) is.character(x) || length(unique(x)) < 10)
data[categorical_columns] <- lapply(data[categorical_columns], factor)

# Fit the multinomial logistic regression model
# 'NObeyesdad' is the target, and all other columns are predictors
multinom_model <- multinom(NObeyesdad ~ ., data = data)

# Summary of the model
summary(multinom_model)

```

$$
DegreesOfFreedom = NumObservations - NumParameters = 2111 - 24 = 2087
$$

Since the target variable NObeyesdad is categorical, we performed
Multinomial logistic regression. The coefficients examine the log-odds
impact of predictors such as gender, age, height, weight, and family
history of overweight. Positive coefficients represent the increased
odds and negative coefficients indicate the decreased odds relative to a
reference group. The 'GenderMale' predictor is positively associated
with higher weight categories, meaning that males are likely to have
increased odds of being in 'Overweight_Level_I' and 'Obesity_Type_I'
categories than reference female group. Regarding the residual deviance,
a lower value generally indicates a better fit. As residual deviance is
significantly smaller than the degree of freedom of 2,087, we assume the
model is a very good fit. Likewise, AIC is also preferable once it has
lower values. The given value of 506.2212, seems competitive enough, but
it has to be further tackled with other models.

## KNN

```{r}

# Fit the kNN model using the base 'knn' function since 'knn3' is not a standard function
set.seed(123) # for reproducible results
knn_fit <- knn(train = X_train,
               test = X_test,
               cl = y_train, k = 5)


#confusion matrix
confusionMatrix <- table(Predicted = knn_fit, Actual = y_test)
confusion_data <- as.data.frame(as.table(confusionMatrix))

# Create the heatmap
ggplot(confusion_data, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") + 
  scale_fill_gradient(low = "steelblue", high = "red") + 
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) + 
  theme_minimal() + 
  labs(x = "Actual Class", y = "Predicted Class", fill = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

(calculate evaludation metrics. Used micro-averaged metrics because some
of the denominators were zero during the computation. This could have
happened because there were classes in the test set for which there were
no predictions being made.)

Through the visualization of k-nearest neighbors classification model,
the heatmap illustrates the model's predictions across various obesity
levels. The diagonal cells represent the correctly classified instances.
The heatmap also implys the higher correct classification counts of
Normal_Weight and the three obesity levels. The model evaluation appears
to encounter the issues with some classes not being predicted, which led
to zero denominators when generating evaluation metrics. In order to
address such issue, micro-averaged metrics were employed as it ensures a
more robust evaluation.

```{r}
# Convert factors to a confusion matrix object
conf_matrix <- confusionMatrix(data = knn_fit, reference = test_set$NObeyesdad)

# Print the overall accuracy
cat("Accuracy:", conf_matrix$overall['Accuracy'], "\n")

# Calculate micro-average metrics 
positive_class <- levels(test_set$NObeyesdad)[1]

micro_precision <- sum(conf_matrix$table[1, 1]) / sum(conf_matrix$table[, 1])
micro_recall <- sum(conf_matrix$table[1, 1]) / sum(conf_matrix$table[1, ])
micro_f1_score <- 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)

cat("Micro-averaged Precision:", micro_precision, "\n")
cat("Micro-averaged Recall:", micro_recall, "\n")
cat("Micro-averaged F1-Score:", micro_f1_score, "\n")
```

As shown, the metrics used to examine the performance of the model
across multiple classes are calculated. The micro-average precision is
perfect with a score of 1, meaning that every instance predicted as a
positive class was likely positive. The micro-average recall is
0.9473684. This suggests that the model was able to retrieve about
94.74% of actual positive instances. The F1-score combines the upper two
aspects into a single metric and the result was 0.972973, proving a
well-balanced precision and recall across the classes. The higher values
of these metrics confirms the good performance of the mdoel.

## Naive Bayes

```{r}
# Naive Bayes model
nb_model <- naiveBayes(X_train, y_train)
nb_predictions <- predict(nb_model, newdata = X_test)

# Create a confusion matrix
confusionMatrix <- table(Predicted = nb_predictions, Actual = y_test)
confusion_data <- as.data.frame(as.table(confusionMatrix))

# Create the heatmap
ggplot(confusion_data, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") + 
  scale_fill_gradient(low = "steelblue", high = "red") + 
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) + 
  theme_minimal() + 
  labs(x = "Actual Class", y = "Predicted Class", fill = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The confusion matrix compares the predicted classes against the actual
classes from the test set. The diagonal cells indicate the number of
correct predictions for each class. We can observe high accuracy in
predictions for Obesity_Type_I with 69 correct predictions and
Obesity_Type_III with 64 predictions. Overweight Level I and
Normal_Weight have shown a noticeable misclassifications with 9
instances of Overweight_Level_I being incorrectly predicted as
Normal_Weight. This heatmap successfully illustrates the strengths and
weaknesses of the model, assuring that it performs well in
distinguishing clear cases of underweight and high obesity, but it
encounters some challenges between overweight and different obesity
types.

```{r}
# Evaluate the model performance
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
precision <- diag(confusionMatrix) / rowSums(confusionMatrix)
recall <- diag(confusionMatrix) / colSums(confusionMatrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("\nAccuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
```

The overall accuracy of the model is 0.9619048, promoting a high level
of correctness in predictions. Precision values range from 0.9473684 to
1, reflecting the model's ability to identify true positives among all
positive predictions, with perfect precision for some classes. Recall
range from 0.7894737 to 1, examining the success in identifying all
actual positives, with complete recall for some classes. Fianlly,
F1-scores are between 0.8823529 and 0.991453, meaning that the model has
a balanced performance across the classes. The higher values within
these statistics supprots the model's consist and strong predictive
capability.

## SVM 

SVM might be suitable for obesity data especially if it's not linearly
separable -\> check this

```{r, warning=FALSE}

# Fit SVM model

svm_model <- svm(x = X_train, y = y_train, type="C-classification", kernel="radial")

# Make predictions on the test set
svm_predictions <- predict(svm_model, newdata = X_test)

# Create a confusion matrix
confusionMatrix <- table(Predicted = svm_predictions, Actual = y_test)
confusion_data <- as.data.frame(as.table(confusionMatrix))


# Create the heatmap
ggplot(confusion_data, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") + 
  scale_fill_gradient(low = "steelblue", high = "red") + 
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) + 
  theme_minimal() + 
  labs(x = "Actual Class", y = "Predicted Class", fill = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The matrix illustrates the classifier's performance in categorizing
individuals across six distinct classes. The correct predictions are as
follows: 53 for Insufficient_Weight, 57 for Normal_Weight, 56 for
Overweight_Level_I, 58 for Overweight_Level_II, 58 for Obesity_Type_I,
and 64 for Obesity_Type_III. These results support our initial
supposition that an SVM is adept at handling non-linearly separable data
such as this. The matrix's diagonal dominance, including the minimal
misclassifications, such as only one instance each between
Obesity_Type_I and Obesity_Type_II, demonstrates the capability of SVM
to discern between the classes effectively, validating its application
to obesity level classification where clear linear boundaries may not
exist.

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)

# Calculate precision and recall for each class
precision <- diag(confusionMatrix) / rowSums(confusionMatrix)
recall <- diag(confusionMatrix) / colSums(confusionMatrix)

# Calculate F1-score for each class
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("\nAccuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
```

The accuracy is high with the value of approximately 0.988, indicating
that the model may correctly predict 98.8% of the outcomes. The
precision values from 0.9508197 to 1 suggests that when the model
predicts a class, it is correct between 95.08% and 100% of the time. The
recall values varying from 0.9655172 to indicates the fact that model
successfully identifies between 96.55% and 100% of all positive
instances for each class. The F1-scores, which balance precision and
recall, are also high, ranging from 0.9747899 to 1. This demonstrates
that the model has a robust performance, with a consistent ability to
accurately classify instances across different classes without
significant bias or error.

## Decision Tree 

In the context of obesity data, which includes a combination of
lifestyle factors, medical measurements, and demographic information,
Decision Trees can be quite useful. They can handle both numerical and
categorical data and are capable of modeling complex relationships by
dividing the space into a hierarchy of decisions.

```{r}
# Fit Decision Tree model
y_train <- as.factor(y_train)
train_data <- data.frame(y_train, X_train)
dt_model <- rpart(y_train ~ ., data = train_data, method="class")
test_data <- data.frame(y_test, X_test)
dt_predictions <- predict(dt_model, newdata = test_data, type = "class")

# Create a confusion matrix
confusionMatrix <- table(Predicted = dt_predictions, Actual = test_data$y_test)
confusion_data <- as.data.frame(as.table(confusionMatrix))

# Create the heatmap
ggplot(confusion_data, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "steelblue", high = "red") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  theme_minimal() +
  labs(x = "Actual Class", y = "Predicted Class", fill = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The diagonal values, 54, 57, 58, 70, 59, and 64 for categories,
respectively, indicate the number of correct predictions. The absence of
off-diagonal numbers, meaning the besides the diagonal values, all the
other values within the matrix is zero, proposes no misclassifications,
suggesting a perfect classification with 100% accuracy for this test
set.

Possible limitation: However, it is important to consider the
possibility of overfitting, as such high accuracy is unusual.

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)

# Calculate precision and recall for each class
precision <- diag(confusionMatrix) / rowSums(confusionMatrix)
recall <- diag(confusionMatrix) / colSums(confusionMatrix)

# Calculate F1-score for each class
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("\nAccuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")

```

The overall accuracy of the model is 0.9880952, so 98.81% of the
outcomes are likely to be predicted correctly by the model. Precision
values for individual classes varying between 0.9508197 and 1 suggests a
high likelihood that the model's positive predictions are correct.
Recall values, which range from 0.9655172 to 1, indicates that the model
is highly capable of detecting true positives. The F1-Score presents
high values between 0.9747899 and 1, confirming that the model
demonstrates a robust performance.

```{r, warning=FALSE}
# Visualize the tree
library(rpart.plot)
rpart.plot(dt_model)
```

## random forest

although the decision tree works well, since the tree is too accurate
with accuracy, precision, recall and F1 equal to 1.0, it might indicate
overfitting. So we can use random forest ensembling to tackle this.

```{r}
rf_model <- randomForest(y_train ~ ., data = train_data, ntree=500, importance=TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)

# Create a confusion matrix
confusionMatrix <- table(Predicted = rf_predictions, Actual = test_data$y_test)
confusion_data <- as.data.frame(as.table(confusionMatrix))

# Create the heatmap
ggplot(confusion_data, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") + 
  scale_fill_gradient(low = "steelblue", high = "red") + 
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) + 
  theme_minimal() + 
  labs(x = "Actual Class", y = "Predicted Class", fill = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

The diagonal cells within this matrix from random forest show a high
number of correct predictions for each class with no misclassifications,
evidenced by zero values in all off-diagonal cells. This pattern
suggests an exemplary classification performance. However, this perfect
classification performance, mirroring the previously mentioned decision
tree's metrics, may still raise concerns regarding the potential
overfitting. The usage of a random forest ensemble model, which is
designed to improve generality by integrating multiple decision trees,
is typically employed to alleviate the overfitting risks.

```{r}
accuracy <- sum(diag(confusionMatrix)) / sum(confusionMatrix)
precision <- diag(confusionMatrix) / rowSums(confusionMatrix)
recall <- diag(confusionMatrix) / colSums(confusionMatrix)
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print the metrics
cat("\nAccuracy:", accuracy, "\n")
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
```

Regarding the above statistics, accuracy, precision, recall, and
F1-score are all equal to 1. This suggests the perfect performance of
the classifier across all classes. Accuracy of 1 indicates that every
instance was correctly classified and a precision of 1 implys that every
prediction made for each class was correct. A recall of 1 for all
classes denotes that all actual instances of each class were identified
correctly, with no true instances missed. Finally, an F1-score of 1,
confirms the model's flawless performance in both precision and recall
for each class..

```{r}
# Get variable importance
importance(rf_model)
```
